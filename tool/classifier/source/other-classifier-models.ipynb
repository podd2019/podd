{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "#from matplotlib.mlab import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data = my_data = np.genfromtxt('full_pcm.results', delimiter=',')\n",
    "#remove header and non-feature column\n",
    "X = data[1:,4:]\n",
    "Y = data[1:,3]\n",
    "\n",
    "X = normalize(X, axis=0, norm='l2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gamma_list=[10, 20, 40]\n",
    "C_list=[10]\n",
    "# cross validation \n",
    "for g in gamma_list:\n",
    "    for c in C_list:\n",
    "        true_positive_count= 0\n",
    "        false_positive_count= 0\n",
    "        false_negative_count= 0\n",
    "        for i in range(28):\n",
    "            train_X = np.concatenate((X[:i*8,:],X[i*8+8:,:]))\n",
    "            test_X = X[i*8:i*8+8,:]\n",
    "            train_Y = np.concatenate((Y[:i*8],Y[i*8+8:]))\n",
    "            test_Y = Y[i*8:i*8+8]\n",
    "            pca = PCA(n_components=5)\n",
    "            pca.fit(train_X)\n",
    "            #print(pca.explained_variance_ratio_)\n",
    "\n",
    "            transformed_train_X = pca.transform(train_X)\n",
    "            transformed_test_X = pca.transform(test_X)\n",
    "            #pca.components_\n",
    "\n",
    "            #learning\n",
    "\n",
    "            #learner = svm.SVC(kernel='linear', gamma=g, C=c, class_weight={0:0.5, 1:0.8})\n",
    "            #learner = RandomForestClassifier(n_estimators=c, n_jobs=7, max_depth=g,class_weight={0:0.5, 1:5})\n",
    "            #learner = KNeighborsClassifier(n_neighbors=20)\n",
    "            #learner = AdaBoostClassifier(DecisionTreeClassifier(max_depth=10,class_weight={0:0.5, 1:5}), algorithm=\"SAMME\", n_estimators=1000)\n",
    "            #learner = ExtraTreesClassifier(n_estimators=10, n_jobs=7, criterion='entropy', class_weight={0:0.5, 1:10})\n",
    "            learner = LogisticRegression(solver='lbfgs', class_weight={0:1, 1:1.8})\n",
    "            learner.fit(transformed_train_X,train_Y)\n",
    "\n",
    "            predicted=learner.predict(transformed_test_X)\n",
    "            diff = predicted - test_Y\n",
    "\n",
    "            for index in range(len(predicted)):\n",
    "                if predicted[index] ==0 and test_Y[index] ==0:\n",
    "                    true_positive_count +=1\n",
    "                if predicted[index] ==0 and test_Y[index] ==1:\n",
    "                    false_positive_count +=1\n",
    "                if predicted[index] ==1 and test_Y[index] ==0:\n",
    "                    false_negative_count += 1\n",
    "            #print(np.absolute(diff))\n",
    "        print(\"C = \" + str(c) + \", gamma = \" + str(g))\n",
    "        print(\"recall = \" + str(true_positive_count/(true_positive_count+false_negative_count)))\n",
    "        print(\"precision = \" + str(true_positive_count / (true_positive_count + false_positive_count)))\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
